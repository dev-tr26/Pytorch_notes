{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Autograd : pytorch module to compute automatic differntiation for tensor computations. it enables gradient computing used in optimization algorithms like gradient descent."
      ],
      "metadata": {
        "id": "u0ih1zutSk5Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xo_nmq-1-cBw"
      },
      "outputs": [],
      "source": [
        "def dy_dx(x):\n",
        "  return 2*x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDwBSc72TNYr",
        "outputId": "95e120e2-02bc-47a3-fd71-ebe756dfc35d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(4.0, requires_grad=True) # it tells pytorch that we want to comput its gradient\n",
        "x\n",
        "\n",
        "# internally it makes a computation graph for calculating gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAu2_lEpTNbZ",
        "outputId": "d8755ac4-0ea3-411e-8f7e-d5c745872ae9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZIpinCfTNd6",
        "outputId": "85527fad-e7ca-476d-cba3-eca1813688d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# o/p tensor.backward()\n",
        "\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "9LYTrpuITNgp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad  # to see the gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy4aGs49TNja",
        "outputId": "38f514ba-f919-487b-cb6b-496b0b2900a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(16.0, requires_grad=True)  # requires floatingpoint only\n",
        "b = a **2\n",
        "c = torch.sin(b)"
      ],
      "metadata": {
        "id": "j6gzU8WiTNmY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar8cjxKAUnao",
        "outputId": "77d9e1c1-a500-4ea8-bb48-191fc0596440"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(16., requires_grad=True)\n",
            "tensor(256., grad_fn=<PowBackward0>)\n",
            "tensor(-0.9992, grad_fn=<SinBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c.backward()"
      ],
      "metadata": {
        "id": "cNOS6mhBVQHc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3XKL0NpUnXn",
        "outputId": "303000ff-50ef-43a2-e70b-cd9b7d777b95"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.2733)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# derivatives are only calculated for root nodes / leafnodes not intermediate nodes in dags"
      ],
      "metadata": {
        "id": "0H6pXROCUnVI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training simplest nn and calculating gradient"
      ],
      "metadata": {
        "id": "ilJcB6qgX5Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs\n",
        "\n",
        "x = torch.tensor(6.7)  # input feature\n",
        "y = torch.tensor(0.0)  # True label\n",
        "\n",
        "w = torch.tensor(1.0) # weight\n",
        "b = torch.tensor(0.0)   # bias"
      ],
      "metadata": {
        "id": "cJJtdOt8UnSH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Cross Entropy Loss for Scalar\n",
        "\n",
        "def binary_cross_entropy_loss(y_pred, target):\n",
        "  epsilon = 1e-8  # to prevent log(0)\n",
        "  y_pred = torch.clamp(y_pred, epsilon, 1-epsilon)\n",
        "  return -(target * torch.log(y_pred) + (1-target) * torch.log(1-y_pred))"
      ],
      "metadata": {
        "id": "j74_ht3gUnPY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Pass\n",
        "\n",
        "z = w * x + b   # weighted sum (linear part)\n",
        "y_pred = torch.sigmoid(z)  # activation fun predicted probability\n",
        "\n",
        "loss = binary_cross_entropy_loss(y_pred, y)"
      ],
      "metadata": {
        "id": "oC39aV45UnMq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umv_7kxdaG5I",
        "outputId": "9188f54d-c482-4478-d788-700efd182d3a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivatives\n",
        "# 1. dL/d(y_pred)    # loss w.r.t prediction\n",
        "dL_dP = (y_pred - y)/ (y_pred *(1- y_pred))\n",
        "\n",
        "#2. dy_pred/ dz :    # prediction w.r.t activation fun\n",
        "dy_dz = y_pred * (1- y_pred)\n",
        "\n",
        "#3. dz/dW & dz/db    # z  w.r.t   weights , bias\n",
        "dz_dW = x\n",
        "dz_db = 1            # bias contributes directly to z)\n",
        "\n",
        "dL_dW = dL_dP * dy_dz * dz_dW\n",
        "dL_db = dL_dP * dy_dz * dz_db"
      ],
      "metadata": {
        "id": "XFieYEMcUnJo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"manual Gradient of loss w.r.t weights: {dL_dW}\")\n",
        "print(f\"manual Gradient of loss w.r.t bias: {dL_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVEYjW4jUnGV",
        "outputId": "eeffda6c-27e8-4fc2-d0a8-2ecd6a6a951f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "manual Gradient of loss w.r.t weights: 6.691762447357178\n",
            "manual Gradient of loss w.r.t bias: 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "1kmwVA2Mb5Nv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "7JE8svJGb5Ku"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = w*x + b\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZORXiBVb5Hv",
        "outputId": "6f71d712-4936-44e2-eb56-4f380d61569b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l01YstZuc_He",
        "outputId": "5542fac6-9dcd-46ca-9f9c-68885485a4eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY_chr-vdFwQ",
        "outputId": "f4fb20dc-b1c6-464d-8aa3-9372be01f386"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "-5A7nJMFb5EO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOnTrLGZb5Bf",
        "outputId": "c871b8e6-fd84-4500-ad5a-292b80ce632a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for vector inputs\n",
        "\n",
        "x = torch.tensor([3.8, 6.7, 8.9],requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIiWeQuTdogM",
        "outputId": "c55a15d0-2aec-4116-d5cd-142a32e80b99"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.8000, 6.7000, 8.9000], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = (x ** 3 + 3 * (x**2) - 4 * x + 5 ).mean()"
      ],
      "metadata": {
        "id": "lIMnYtbpdocc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "RqGVcZVidoZp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMz0vGpfdoWb",
        "outputId": "d957eb65-cf9d-4a02-d87a-06c0ac6e9b29"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 62.1200, 170.8700, 287.0300])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clearing grad\n",
        "\n",
        "# when muiltiple times backward pass done gradients start accumulating\n",
        "# i.e. 1st time its 3 then becomes 6 like this earlier pass gradients add to current grad\n",
        "# so clear each time at last  using .grad.zero_()"
      ],
      "metadata": {
        "id": "86VoFQkzdoTa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0,requires_grad=True)\n",
        "for i in range(400):\n",
        "  y = x**2\n",
        "  y.backward()\n",
        "  print(x.grad)\n",
        "  # x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU_WnOhcdoN6",
        "outputId": "9809c1af-e866-47ad-f557-f1c7adb4c85b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n",
            "tensor(8.)\n",
            "tensor(12.)\n",
            "tensor(16.)\n",
            "tensor(20.)\n",
            "tensor(24.)\n",
            "tensor(28.)\n",
            "tensor(32.)\n",
            "tensor(36.)\n",
            "tensor(40.)\n",
            "tensor(44.)\n",
            "tensor(48.)\n",
            "tensor(52.)\n",
            "tensor(56.)\n",
            "tensor(60.)\n",
            "tensor(64.)\n",
            "tensor(68.)\n",
            "tensor(72.)\n",
            "tensor(76.)\n",
            "tensor(80.)\n",
            "tensor(84.)\n",
            "tensor(88.)\n",
            "tensor(92.)\n",
            "tensor(96.)\n",
            "tensor(100.)\n",
            "tensor(104.)\n",
            "tensor(108.)\n",
            "tensor(112.)\n",
            "tensor(116.)\n",
            "tensor(120.)\n",
            "tensor(124.)\n",
            "tensor(128.)\n",
            "tensor(132.)\n",
            "tensor(136.)\n",
            "tensor(140.)\n",
            "tensor(144.)\n",
            "tensor(148.)\n",
            "tensor(152.)\n",
            "tensor(156.)\n",
            "tensor(160.)\n",
            "tensor(164.)\n",
            "tensor(168.)\n",
            "tensor(172.)\n",
            "tensor(176.)\n",
            "tensor(180.)\n",
            "tensor(184.)\n",
            "tensor(188.)\n",
            "tensor(192.)\n",
            "tensor(196.)\n",
            "tensor(200.)\n",
            "tensor(204.)\n",
            "tensor(208.)\n",
            "tensor(212.)\n",
            "tensor(216.)\n",
            "tensor(220.)\n",
            "tensor(224.)\n",
            "tensor(228.)\n",
            "tensor(232.)\n",
            "tensor(236.)\n",
            "tensor(240.)\n",
            "tensor(244.)\n",
            "tensor(248.)\n",
            "tensor(252.)\n",
            "tensor(256.)\n",
            "tensor(260.)\n",
            "tensor(264.)\n",
            "tensor(268.)\n",
            "tensor(272.)\n",
            "tensor(276.)\n",
            "tensor(280.)\n",
            "tensor(284.)\n",
            "tensor(288.)\n",
            "tensor(292.)\n",
            "tensor(296.)\n",
            "tensor(300.)\n",
            "tensor(304.)\n",
            "tensor(308.)\n",
            "tensor(312.)\n",
            "tensor(316.)\n",
            "tensor(320.)\n",
            "tensor(324.)\n",
            "tensor(328.)\n",
            "tensor(332.)\n",
            "tensor(336.)\n",
            "tensor(340.)\n",
            "tensor(344.)\n",
            "tensor(348.)\n",
            "tensor(352.)\n",
            "tensor(356.)\n",
            "tensor(360.)\n",
            "tensor(364.)\n",
            "tensor(368.)\n",
            "tensor(372.)\n",
            "tensor(376.)\n",
            "tensor(380.)\n",
            "tensor(384.)\n",
            "tensor(388.)\n",
            "tensor(392.)\n",
            "tensor(396.)\n",
            "tensor(400.)\n",
            "tensor(404.)\n",
            "tensor(408.)\n",
            "tensor(412.)\n",
            "tensor(416.)\n",
            "tensor(420.)\n",
            "tensor(424.)\n",
            "tensor(428.)\n",
            "tensor(432.)\n",
            "tensor(436.)\n",
            "tensor(440.)\n",
            "tensor(444.)\n",
            "tensor(448.)\n",
            "tensor(452.)\n",
            "tensor(456.)\n",
            "tensor(460.)\n",
            "tensor(464.)\n",
            "tensor(468.)\n",
            "tensor(472.)\n",
            "tensor(476.)\n",
            "tensor(480.)\n",
            "tensor(484.)\n",
            "tensor(488.)\n",
            "tensor(492.)\n",
            "tensor(496.)\n",
            "tensor(500.)\n",
            "tensor(504.)\n",
            "tensor(508.)\n",
            "tensor(512.)\n",
            "tensor(516.)\n",
            "tensor(520.)\n",
            "tensor(524.)\n",
            "tensor(528.)\n",
            "tensor(532.)\n",
            "tensor(536.)\n",
            "tensor(540.)\n",
            "tensor(544.)\n",
            "tensor(548.)\n",
            "tensor(552.)\n",
            "tensor(556.)\n",
            "tensor(560.)\n",
            "tensor(564.)\n",
            "tensor(568.)\n",
            "tensor(572.)\n",
            "tensor(576.)\n",
            "tensor(580.)\n",
            "tensor(584.)\n",
            "tensor(588.)\n",
            "tensor(592.)\n",
            "tensor(596.)\n",
            "tensor(600.)\n",
            "tensor(604.)\n",
            "tensor(608.)\n",
            "tensor(612.)\n",
            "tensor(616.)\n",
            "tensor(620.)\n",
            "tensor(624.)\n",
            "tensor(628.)\n",
            "tensor(632.)\n",
            "tensor(636.)\n",
            "tensor(640.)\n",
            "tensor(644.)\n",
            "tensor(648.)\n",
            "tensor(652.)\n",
            "tensor(656.)\n",
            "tensor(660.)\n",
            "tensor(664.)\n",
            "tensor(668.)\n",
            "tensor(672.)\n",
            "tensor(676.)\n",
            "tensor(680.)\n",
            "tensor(684.)\n",
            "tensor(688.)\n",
            "tensor(692.)\n",
            "tensor(696.)\n",
            "tensor(700.)\n",
            "tensor(704.)\n",
            "tensor(708.)\n",
            "tensor(712.)\n",
            "tensor(716.)\n",
            "tensor(720.)\n",
            "tensor(724.)\n",
            "tensor(728.)\n",
            "tensor(732.)\n",
            "tensor(736.)\n",
            "tensor(740.)\n",
            "tensor(744.)\n",
            "tensor(748.)\n",
            "tensor(752.)\n",
            "tensor(756.)\n",
            "tensor(760.)\n",
            "tensor(764.)\n",
            "tensor(768.)\n",
            "tensor(772.)\n",
            "tensor(776.)\n",
            "tensor(780.)\n",
            "tensor(784.)\n",
            "tensor(788.)\n",
            "tensor(792.)\n",
            "tensor(796.)\n",
            "tensor(800.)\n",
            "tensor(804.)\n",
            "tensor(808.)\n",
            "tensor(812.)\n",
            "tensor(816.)\n",
            "tensor(820.)\n",
            "tensor(824.)\n",
            "tensor(828.)\n",
            "tensor(832.)\n",
            "tensor(836.)\n",
            "tensor(840.)\n",
            "tensor(844.)\n",
            "tensor(848.)\n",
            "tensor(852.)\n",
            "tensor(856.)\n",
            "tensor(860.)\n",
            "tensor(864.)\n",
            "tensor(868.)\n",
            "tensor(872.)\n",
            "tensor(876.)\n",
            "tensor(880.)\n",
            "tensor(884.)\n",
            "tensor(888.)\n",
            "tensor(892.)\n",
            "tensor(896.)\n",
            "tensor(900.)\n",
            "tensor(904.)\n",
            "tensor(908.)\n",
            "tensor(912.)\n",
            "tensor(916.)\n",
            "tensor(920.)\n",
            "tensor(924.)\n",
            "tensor(928.)\n",
            "tensor(932.)\n",
            "tensor(936.)\n",
            "tensor(940.)\n",
            "tensor(944.)\n",
            "tensor(948.)\n",
            "tensor(952.)\n",
            "tensor(956.)\n",
            "tensor(960.)\n",
            "tensor(964.)\n",
            "tensor(968.)\n",
            "tensor(972.)\n",
            "tensor(976.)\n",
            "tensor(980.)\n",
            "tensor(984.)\n",
            "tensor(988.)\n",
            "tensor(992.)\n",
            "tensor(996.)\n",
            "tensor(1000.)\n",
            "tensor(1004.)\n",
            "tensor(1008.)\n",
            "tensor(1012.)\n",
            "tensor(1016.)\n",
            "tensor(1020.)\n",
            "tensor(1024.)\n",
            "tensor(1028.)\n",
            "tensor(1032.)\n",
            "tensor(1036.)\n",
            "tensor(1040.)\n",
            "tensor(1044.)\n",
            "tensor(1048.)\n",
            "tensor(1052.)\n",
            "tensor(1056.)\n",
            "tensor(1060.)\n",
            "tensor(1064.)\n",
            "tensor(1068.)\n",
            "tensor(1072.)\n",
            "tensor(1076.)\n",
            "tensor(1080.)\n",
            "tensor(1084.)\n",
            "tensor(1088.)\n",
            "tensor(1092.)\n",
            "tensor(1096.)\n",
            "tensor(1100.)\n",
            "tensor(1104.)\n",
            "tensor(1108.)\n",
            "tensor(1112.)\n",
            "tensor(1116.)\n",
            "tensor(1120.)\n",
            "tensor(1124.)\n",
            "tensor(1128.)\n",
            "tensor(1132.)\n",
            "tensor(1136.)\n",
            "tensor(1140.)\n",
            "tensor(1144.)\n",
            "tensor(1148.)\n",
            "tensor(1152.)\n",
            "tensor(1156.)\n",
            "tensor(1160.)\n",
            "tensor(1164.)\n",
            "tensor(1168.)\n",
            "tensor(1172.)\n",
            "tensor(1176.)\n",
            "tensor(1180.)\n",
            "tensor(1184.)\n",
            "tensor(1188.)\n",
            "tensor(1192.)\n",
            "tensor(1196.)\n",
            "tensor(1200.)\n",
            "tensor(1204.)\n",
            "tensor(1208.)\n",
            "tensor(1212.)\n",
            "tensor(1216.)\n",
            "tensor(1220.)\n",
            "tensor(1224.)\n",
            "tensor(1228.)\n",
            "tensor(1232.)\n",
            "tensor(1236.)\n",
            "tensor(1240.)\n",
            "tensor(1244.)\n",
            "tensor(1248.)\n",
            "tensor(1252.)\n",
            "tensor(1256.)\n",
            "tensor(1260.)\n",
            "tensor(1264.)\n",
            "tensor(1268.)\n",
            "tensor(1272.)\n",
            "tensor(1276.)\n",
            "tensor(1280.)\n",
            "tensor(1284.)\n",
            "tensor(1288.)\n",
            "tensor(1292.)\n",
            "tensor(1296.)\n",
            "tensor(1300.)\n",
            "tensor(1304.)\n",
            "tensor(1308.)\n",
            "tensor(1312.)\n",
            "tensor(1316.)\n",
            "tensor(1320.)\n",
            "tensor(1324.)\n",
            "tensor(1328.)\n",
            "tensor(1332.)\n",
            "tensor(1336.)\n",
            "tensor(1340.)\n",
            "tensor(1344.)\n",
            "tensor(1348.)\n",
            "tensor(1352.)\n",
            "tensor(1356.)\n",
            "tensor(1360.)\n",
            "tensor(1364.)\n",
            "tensor(1368.)\n",
            "tensor(1372.)\n",
            "tensor(1376.)\n",
            "tensor(1380.)\n",
            "tensor(1384.)\n",
            "tensor(1388.)\n",
            "tensor(1392.)\n",
            "tensor(1396.)\n",
            "tensor(1400.)\n",
            "tensor(1404.)\n",
            "tensor(1408.)\n",
            "tensor(1412.)\n",
            "tensor(1416.)\n",
            "tensor(1420.)\n",
            "tensor(1424.)\n",
            "tensor(1428.)\n",
            "tensor(1432.)\n",
            "tensor(1436.)\n",
            "tensor(1440.)\n",
            "tensor(1444.)\n",
            "tensor(1448.)\n",
            "tensor(1452.)\n",
            "tensor(1456.)\n",
            "tensor(1460.)\n",
            "tensor(1464.)\n",
            "tensor(1468.)\n",
            "tensor(1472.)\n",
            "tensor(1476.)\n",
            "tensor(1480.)\n",
            "tensor(1484.)\n",
            "tensor(1488.)\n",
            "tensor(1492.)\n",
            "tensor(1496.)\n",
            "tensor(1500.)\n",
            "tensor(1504.)\n",
            "tensor(1508.)\n",
            "tensor(1512.)\n",
            "tensor(1516.)\n",
            "tensor(1520.)\n",
            "tensor(1524.)\n",
            "tensor(1528.)\n",
            "tensor(1532.)\n",
            "tensor(1536.)\n",
            "tensor(1540.)\n",
            "tensor(1544.)\n",
            "tensor(1548.)\n",
            "tensor(1552.)\n",
            "tensor(1556.)\n",
            "tensor(1560.)\n",
            "tensor(1564.)\n",
            "tensor(1568.)\n",
            "tensor(1572.)\n",
            "tensor(1576.)\n",
            "tensor(1580.)\n",
            "tensor(1584.)\n",
            "tensor(1588.)\n",
            "tensor(1592.)\n",
            "tensor(1596.)\n",
            "tensor(1600.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0,requires_grad=True)\n",
        "for i in range(400):\n",
        "  y = x**2\n",
        "  y.backward()\n",
        "  print(x.grad)\n",
        "  x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-fpQgh8doKX",
        "outputId": "7f885d42-1b8e-4262-87fc-fbcfee80c2b1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to disable gradient tracking"
      ],
      "metadata": {
        "id": "QQrLPCCFw7yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need when grad tracking is not required\n",
        "# grad cal needed while forward and backward pass while training nn\n",
        "# when we're doing predictions then not needed as it occupies memory\n",
        "# so we off it while backtracking and not forward pass"
      ],
      "metadata": {
        "id": "_yJGaa5Rb4-f"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. requires_grad_(False)\n",
        "# 2. detach()\n",
        "# 3. torch.no_grad()"
      ],
      "metadata": {
        "id": "bXLDHlunw7f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad\n",
        "x.requires_grad_(False)\n"
      ],
      "metadata": {
        "id": "JpjJltQ7w7dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0,requires_grad=True)\n",
        "z = x.detach()  # this is detached from computational graph and gradient tracking is off\n",
        "y = x**2\n",
        "y1 = z**2\n",
        "print(x)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idrbGqt1w7Zd",
        "outputId": "7d857f96-7cd4-4fb9-9274-009ff43c7f4e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n",
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfP4VwZow7V_",
        "outputId": "8169201b-8360-4f47-9db0-20d133e653ce"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y1.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "nmK18SnOw7P3",
        "outputId": "7bf6f3b5-78d8-44de-b8ee-87c18a18a11a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-78ed6cb36aaa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y = x**2"
      ],
      "metadata": {
        "id": "W5Dwr797y-lk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "llwy1claw7NI",
        "outputId": "5b03c3ac-3927-4eaf-f5c2-3c4bd4f15fc3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-ab75bb780f4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    }
  ]
}